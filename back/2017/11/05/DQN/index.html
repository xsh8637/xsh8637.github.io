<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="强化学习," />










<meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;]]} });    一、 概述 强化学习算法可以分为三大类：value based, policy based 和 actor critic。常见的是以DQN为代表的value based算法，这种算法中只有一个值函数网络，没有policy网络，以及以DDPG,">
<meta name="keywords" content="强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习—DQN算法原理详解">
<meta property="og:url" content="https://wanjun0511.github.io/2017/11/05/DQN/index.html">
<meta property="og:site_name" content="Wanjun&#39;s blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;]]} });    一、 概述 强化学习算法可以分为三大类：value based, policy based 和 actor critic。常见的是以DQN为代表的value based算法，这种算法中只有一个值函数网络，没有policy网络，以及以DDPG,">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://wanjun0511.github.io/2017/11/05/DQN/network.jpeg">
<meta property="og:image" content="https://wanjun0511.github.io/2017/11/05/DQN/MC.png">
<meta property="og:image" content="https://wanjun0511.github.io/2017/11/05/DQN/TD.png">
<meta property="og:image" content="https://wanjun0511.github.io/2017/11/05/DQN/sarsa.png">
<meta property="og:image" content="https://wanjun0511.github.io/2017/11/05/DQN/qlearning.png">
<meta property="og:image" content="https://wanjun0511.github.io/2017/11/05/DQN/dqn.png">
<meta property="og:updated_time" content="2018-03-04T10:24:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习—DQN算法原理详解">
<meta name="twitter:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&apos;$&apos;,&apos;$&apos;], [&apos;\\(&apos;,&apos;\\)&apos;]]} });    一、 概述 强化学习算法可以分为三大类：value based, policy based 和 actor critic。常见的是以DQN为代表的value based算法，这种算法中只有一个值函数网络，没有policy网络，以及以DDPG,">
<meta name="twitter:image" content="https://wanjun0511.github.io/2017/11/05/DQN/network.jpeg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://wanjun0511.github.io/2017/11/05/DQN/"/>





  <title>强化学习—DQN算法原理详解 | Wanjun's blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wanjun's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">念念不忘，必有回响</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-主页">
          <a href="/" rel="section">
            
            主页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-强化学习">
          <a href="/tags/强化学习" rel="section">
            
            强化学习
          </a>
        </li>
      
        
        <li class="menu-item menu-item-更博计划">
          <a href="/tags/更博计划" rel="section">
            
            更博计划
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wanjun0511.github.io/2017/11/05/DQN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wanjun">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/img/photo.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wanjun's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">强化学习—DQN算法原理详解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-05T12:57:00+08:00">
                2017-11-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DQN/" itemprop="url" rel="index">
                    <span itemprop="name">DQN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/05/DQN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2017/11/05/DQN/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<h2 id="一-概述">一、 概述</h2>
<p>强化学习算法可以分为三大类：value based, policy based 和 actor critic。常见的是以DQN为代表的value based算法，这种算法中只有一个值函数网络，没有policy网络，以及以DDPG,TRPO为代表的actor-critic算法，这种算法中既有值函数网络，又有policy网络。</p>
<p>说到DQN中有值函数网络，这里简单介绍一下强化学习中的一个概念，叫<strong>值函数近似</strong>。在<a href="https://wanjun0511.github.io/2017/11/04/强化学习-基本概念/">基本概念</a>这篇中有讲过，一个state action pair <span class="math inline">\((s, a)\)</span>对应一个值函数<span class="math inline">\(Q(s, a)\)</span>。理论上对于任意的<span class="math inline">\((s, a)\)</span>我们都可以由公式求出它的值函数，即用一个查询表lookup table来表示值函数。但是当state或action的个数过多时，分别去求每一个值函数会很慢。因此我们用函数近似的方式去估计值函数： <span class="math display">\[\hat{Q}(s, a, w) \approx Q_\pi(s, a)\]</span></p>
<a id="more"></a>
<p>这样，对于未出现的state action也可以估计值函数。<br>
至于近似函数，DQN中用的是神经网络，当然如果环境比较简单的话用线性函数来近似也是可以的。</p>
<p>DQN算法原文链接： <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="external">2013版(arxiv)</a> <a href="http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html" target="_blank" rel="external">2015版(nature)</a></p>
<h2 id="二算法原理">二、算法原理</h2>
<p>在<a href="https://wanjun0511.github.io/2017/11/04/强化学习-基本概念/">基本概念</a>中有说过，强化学习是一个反复迭代的过程，每一次迭代要解决两个问题：给定一个策略求值函数，和根据值函数来更新策略。</p>
<p>上面说过DQN使用神经网络来近似值函数，即神经网络的输入是state <span class="math inline">\(s\)</span>,输出是<span class="math inline">\(Q(s, a), \forall a \in \mathcal{A}\)</span> (action space)。通过神经网络计算出值函数后，DQN使用<span class="math inline">\(\epsilon-greedy\)</span>策略来输出action（第四部分中介绍）。值函数网络与<span class="math inline">\(\epsilon-greedy\)</span>策略之间的联系是这样的：首先环境会给出一个obs，智能体根据值函数网络得到关于这个obs的所有<span class="math inline">\(Q(s, a)\)</span>，然后利用<span class="math inline">\(\epsilon-greedy\)</span>选择action并做出决策，环境接收到此action后会给出一个奖励Rew及下一个obs。这是一个step。此时我们根据Rew去更新值函数网络的参数。接着进入下一个step。如此循环下去，直到我们训练出了一个好的值函数网络。</p>
<img src="/2017/11/05/DQN/network.jpeg" title="DQN-network">
<p>那么每次迭代如何更新神经网络的参数呢？</p>
<p>与机器学习类似，首先会定义一个loss function，然后使用梯度下降GD来更新参数。接下来首先介绍DQN的loss function，它与Q-Learning的非常类似，只是添加了一个target Q function。然后会介绍除此之外，DQN在Q-Learning上所做的改进。</p>
<h3 id="loss-function">1、Loss Function</h3>
<p><span class="math display">\[L(\omega) = E[(R + \gamma \cdot max_{a&#39;}Q(s&#39;, a&#39;; \omega^-) - Q(s,a; \omega))^2]\]</span> 这个公式表面上看起来很复杂，实际上很好理解，它就是一个残差模型，和我们平常见的最小二乘法很类似，真实值与预测值之间的差的平方。预测值就是<span class="math inline">\(Q(s,a; \omega)\)</span>，它是神经网络的输出。“真实值”略微有一点复杂。</p>
<p>想象一下假如我们想求出<span class="math inline">\((s, a)\)</span>的真实值函数<span class="math inline">\(Q(s, a)\)</span>。它表示我从state <span class="math inline">\(s\)</span>开始出发，并采取action <span class="math inline">\(a\)</span>的话，我所能得到的整体收益的期望值。一种可能的情况是，我们知道环境的模型。这在强化学习中也叫Model Based RL。即我们知道状态转移概率矩阵：当处于<span class="math inline">\((s, a)\)</span>时，下一个到达的状态可能是什么，并且到达每一个状态的概率是什么；我们还知道奖励函数：当处于<span class="math inline">\((s, a)\)</span>时，得到的立即回报的期望值是什么；另外还知道折扣因子。由此，我们便可以通过贝尔曼方程来求解值函数。这种情况下我们可能也并不需要神经网络近似值函数之类的，直接由策略迭代或值迭代便可以求出最优策略。具体方法可以看一下MDP。</p>
<p>另一种情况就是Model Free RL：不管有没有环境模型，反正我不用。那么在不知道环境模型的情况下如何求解值函数呢？答案就是<strong>采样</strong>。强化学习中有多种采样的方法，这里简单介绍一下：</p>
<h4 id="monte-carlo">(1) Monte Carlo</h4>
<img src="/2017/11/05/DQN/MC.png" title="MC">
<ul>
<li>MC使用一个完整的episode去更新值函数，因此它需要从<span class="math inline">\(S_t\)</span>到Terminal state的完整样本。<br>
</li>
<li>而且需要等到episode结束才能更新值函数。<br>
</li>
<li>由于有一条完整的样本，它可以计算出return，而值函数是return的期望，所以我们可以用return去更新值函数。</li>
</ul>
<h4 id="temporal-difference-sarsa">(2) Temporal Difference / SarSa</h4>
<img src="/2017/11/05/DQN/TD.png" title="TD">
<ul>
<li>与MC不一样的是，TD不需要完整的样本，它只依赖下一个step的值函数，即它用<span class="math inline">\(V(S_{t+1})\)</span> 去更新 <span class="math inline">\(V(S_t)\)</span> (TD), 或用 <span class="math inline">\(Q(S_{t+1}, a_{t+1})\)</span> 去更新 <span class="math inline">\(Q(S_t, a_t)\)</span> (SarSa)</li>
<li>它不用等到episode结束，每走一步就可以更新值函数。<br>
</li>
<li>它不是用的真实的return值来更新值函数，而是用的一个估计值去更新另一个估计值的思想。</li>
</ul>
<p>另外还有<span class="math inline">\(TD(\lambda)\)</span> (或 <span class="math inline">\(SarSa(\lambda)\)</span>)方法，由于DQN中不涉及，暂不介绍。</p>
<p><strong>DQN属于Model Free的强化学习算法，它需要采样</strong>。且同SarSa类似，只依赖下一个step的值函数。但它更新值函数的方式与SarSa又有所不同。下面介绍从SarSa到Q-Learning再到DQN的更新值函数的方式。</p>
<h4 id="sarsa">(1) SarSa</h4>
<img src="/2017/11/05/DQN/sarsa.png" title="sarsa">
<p>SarSa中更新值函数的公式为： <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma Q(s&#39;, a&#39;) - Q(s, a)]\]</span> 也称它的<strong>target</strong>是 <span class="math inline">\(R_{t+1} + \gamma Q(S_{t+1}, a_{t+1})\)</span>。理解这个target代表的意义： <span class="math display">\[
\begin{equation}
\begin{split}
V(s) &amp;= E[G_t | S_t = s] \\
&amp;= E[R_{t+1} + \gamma G_{t+1} | S_t=s] \\ 
&amp;= E[R_{t+1} + \gamma V(S_{t+1}) | S_t=s]
\end{split}
\nonumber
\end{equation}
\]</span> 将V换成Q的话我们可以看到，SarSa target <span class="math inline">\(R_{t+1}+ \gamma Q(S_{t+1}, a_{t+1})\)</span> 是 <span class="math inline">\(Q(S_t, a_t)\)</span> 的估计值。</p>
<p><strong>在值函数近似中，我们都是使用target来替代真实的值函数，即Loss中的真实值。</strong></p>
<p>前面说过DQN是使用<span class="math inline">\(\epsilon-greedy\)</span>策略来输出action，SarSa和Q-Learning也是。SarSa中使用<span class="math inline">\(\epsilon-greedy\)</span>策略生成action <span class="math inline">\(a_{t+1}\)</span>，随即又用 <span class="math inline">\(a_{t+1}\)</span>处对应的值函数来计算target，更新上一步的值函数。这种学习方式又称为<strong>On-policy</strong>。</p>
<p>SarSa属于On-policy Learning，而Q-Learning属于Off-policy Learning。</p>
<h4 id="q-learning">(2) Q-Learning</h4>
<img src="/2017/11/05/DQN/qlearning.png" title="qlearning">
<p>Q-Learning的target是 <span class="math inline">\(R_{t+1} + \gamma max_{a&#39;} Q(S_{t+1}, a&#39;)\)</span>。它使用 <span class="math inline">\(\epsilon-greedy\)</span>策略来生成action <span class="math inline">\(a_{t+1}\)</span>，但用来计算target的action却不一定是<span class="math inline">\(a_{t+1}\)</span>，而是使得 <span class="math inline">\(Q(S_{t+1}, a)\)</span>最大的action。这种产生行为的策略和进行评估的策略不一样的方法称为Off-policy方法。对于Q-Learning来说，产生行为的策略是<span class="math inline">\(\epsilon-greedy\)</span>，而进行评估的策略是greedy。</p>
<h4 id="dqn">(3) DQN</h4>
<p>Off-policy是Q-Learning的特点，DQN中也延用了这一特点。而不同的是，Q-Learning中用来计算target和预测值的Q是同一个Q，也就是说使用了相同的神经网络。这样带来的一个问题就是，每次更新神经网络的时候，target也都会更新，这样会容易导致参数不收敛。回忆在有监督学习中，标签label都是固定的，不会随着参数的更新而改变。</p>
<p>因此DQN在原来的Q网络的基础上又引入了一个<strong>target Q网络</strong>，即用来计算target的网络。它和Q网络结构一样，初始的权重也一样，只是Q网络每次迭代都会更新，而target Q网络是每隔一段时间才会更新。DQN的target是 <span class="math inline">\(R_{t+1} + \gamma max_{a&#39;} Q(S_{t+1}, a&#39;; \omega^{-})\)</span>。用 <span class="math inline">\(\omega^{-}\)</span>表示它比Q网络的权重 <span class="math inline">\(\omega\)</span>更新得要慢一些。</p>
<p>理解了DQN的target之后也就可以理解DQN的Loss Function了。</p>
<h3 id="dqn所做的改进">2、DQN所做的改进</h3>
<p>相比于Q-Learning，DQN做的改进：一个是使用了卷积神经网络来逼近行为值函数，一个是使用了target Q network来更新target，还有一个是使用了经验回放Experience replay。</p>
<p>由于在强化学习中，我们得到的观测数据是有序的，step by step的，用这样的数据去更新神经网络的参数会有问题。回忆在有监督学习中，数据之间都是独立的。因此DQN中使用经验回放，即用一个Memory来存储经历过的数据，每次更新参数的时候从Memory中抽取一部分的数据来用于更新，以此来打破数据间的关联。</p>
<h2 id="三算法整体流程">三、算法整体流程</h2>
<ul>
<li>首先初始化Memory D，它的容量为N;<br>
</li>
<li>初始化Q网络，随机生成权重<span class="math inline">\(\omega\)</span>;<br>
</li>
<li>初始化target Q网络，权重为<span class="math inline">\(\omega^- = \omega\)</span>;<br>
</li>
<li>循环遍历episode =1, 2, …, M:<br>
</li>
<li>初始化initial state <span class="math inline">\(S_1\)</span>;<br>
</li>
<li>循环遍历step =1,2,…, T:
<ul>
<li>用<span class="math inline">\(\epsilon-greedy\)</span>策略生成action <span class="math inline">\(a_t\)</span>：以<span class="math inline">\(\epsilon\)</span>概率选择一个随机的action，或选择<span class="math inline">\(a_t = max_a Q(S_t, a; \omega)\)</span>;<br>
</li>
<li>执行action <span class="math inline">\(a_t\)</span>，接收reward <span class="math inline">\(r_t\)</span>及新的state <span class="math inline">\(S_{t+1}\)</span>;<br>
</li>
<li>将transition样本 <span class="math inline">\((S_t, a_t, r_t, S_{t+1})\)</span>存入D中；<br>
</li>
<li>从D中随机抽取一个minibatch的transitions <span class="math inline">\((S_j, a_j, r_j, S_{j+1})\)</span>；<br>
</li>
<li>令<span class="math inline">\(y_j = r_j\)</span>，如果 <span class="math inline">\(j+1\)</span>步是terminal的话，否则，令 <span class="math inline">\(y_j = r_j + \gamma max_{a&#39;} Q(S_{t+1}, a&#39;; \omega^-)\)</span>；<br>
</li>
<li>对<span class="math inline">\((y_j - Q(S_t, a_j; \omega))^2\)</span>关于<span class="math inline">\(\omega\)</span>使用梯度下降法进行更新；<br>
</li>
<li>每隔C steps更新target Q网络，<span class="math inline">\(\omega^- = \omega\)</span>。<br>
</li>
</ul></li>
<li>End For;<br>
</li>
<li>End For.</li>
</ul>
<p>附上原文的算法流程：<br>
<img src="/2017/11/05/DQN/dqn.png" title="dqn"></p>
<h2 id="四epsilon-greedy策略">四、<span class="math inline">\(\epsilon-greedy\)</span>策略</h2>
<p><span class="math inline">\(greedy\)</span>策略，顾名思义，是一种贪婪策略，它每次都选择使得值函数最大的action，即<span class="math inline">\(a_t = max_a Q(S_t, a; \omega)\)</span>。但是这种方式有问题，就是对于采样中没有出现过的(state, action) 对，由于没有评估，没有Q值，之后也不会再被采到。</p>
<p>其实这里涉及到了强化学习中一个非常重要的概念，叫Exploration &amp; Exploitation，探索与利用。前者强调发掘环境中的更多信息，并不局限在已知的信息中；后者强调从已知的信息中最大化奖励。而<span class="math inline">\(greedy\)</span>策略只注重了后者，没有涉及前者。所以它并不是一个好的策略。</p>
<p>而<span class="math inline">\(\epsilon-greedy\)</span>策略兼具了探索与利用，它以<span class="math inline">\(\epsilon\)</span>的概率从所有的action中随机抽取一个，以<span class="math inline">\(1- \epsilon\)</span>的概率抽取<span class="math inline">\(a_t = max_a Q(S_t, a; \omega)\)</span>。</p>
<p>强化学习正是因为有了探索Exploration，才会常常有一些出人意表的现象，才会更加与其他机器学习不同。例如智能体在围棋游戏中走出一些人类经验之外的好棋局。</p>
<h2 id="参考资料">参考资料：</h2>
<p>David Silver的课程：www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>谢谢你请我吃糖果</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/img/zhifubao.png" alt="Wanjun 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/04/强化学习-基本概念/" rel="next" title="强化学习-基本概念">
                <i class="fa fa-chevron-left"></i> 强化学习-基本概念
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/19/DDPG/" rel="prev" title="强化学习—DDPG算法原理详解">
                强化学习—DDPG算法原理详解 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/img/photo.png"
                alt="Wanjun" />
            
              <p class="site-author-name" itemprop="name">Wanjun</p>
              <p class="site-description motion-element" itemprop="description"><br><br>nlp与搜索、强化学习等算法开发<br><br>美国UCR统计学硕士<br><br>原创文章，转载请先与我联系!<br><br></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Wanjun0511" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-globe"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:liuwanjunniki@outlook.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.linkedin.com/in/wanjun-liu-8b0131b2/" target="_blank" title="LinkedIn">
                    
                      <i class="fa fa-fw fa-globe"></i>LinkedIn</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一-概述"><span class="nav-text">一、 概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二算法原理"><span class="nav-text">二、算法原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function"><span class="nav-text">1、Loss Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#monte-carlo"><span class="nav-text">(1) Monte Carlo</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#temporal-difference-sarsa"><span class="nav-text">(2) Temporal Difference / SarSa</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sarsa"><span class="nav-text">(1) SarSa</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#q-learning"><span class="nav-text">(2) Q-Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dqn"><span class="nav-text">(3) DQN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dqn所做的改进"><span class="nav-text">2、DQN所做的改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三算法整体流程"><span class="nav-text">三、算法整体流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四epsilon-greedy策略"><span class="nav-text">四、\(\epsilon-greedy\)策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料：</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wanjun</span>

  
</div>









<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: 'Wanjun0511',
            repo: 'Wanjun0511.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: '486078182fc48b44c79ec28eeebcc33dd2821e56',
            
                client_id: '2018295c13033704105f'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    




<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  





  

  

  

  
  

  
  


  

  

</body>
</html>
